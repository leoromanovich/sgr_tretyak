## Параллельные запросы и связанные улучшения

### 1. Асинхронный `llm_client`
- Сделать рядом с текущим `OpenAI` клиентом (`src/llm_client.py:7-90`) экземпляр `AsyncOpenAI`.
- Добавить `async def chat_raw_async(...)` и `async def chat_sgr_parse_async(...)`, повторяющие всю текущую логику (response_format, tools, max_tokens, строгая JSON-схема), но с `await async_client.chat.completions.create`.
- Оставить синхронные функции как thin wrapper поверх `asyncio.run(chat_raw_async(...))`, чтобы существующие вызовы не падали, но вся новая работа шла через `async`.
- На уровне клиента же завести `asyncio.Semaphore`, чтобы можно было централизованно ограничивать число одновременных запросов к vLLM (например, 16), и прокидывать его в воркеры.

### 2. Параллельный проход по заметкам (`scan_people`)
- Переписать `src/tools/scan_people.py:21-82` в `async`-стиле: `scan_people_over_pages_async(workers: int)`.
- Для каждой заметки создавать задачу `process_note(path)`, которая под семафором делает `normalize_people_in_file_async` (эта функция в свою очередь вызывает асинхронные версии метаданных и нормализации, т.е. три последовательных LLM-запроса остаются внутри одного воркера).
- Использовать `asyncio.as_completed` или `TaskGroup`, чтобы по мере готовности заметок сразу писать результаты в JSONL и обновлять прогрессбар.
- Добавить в CLI опцию `--workers` (по умолчанию 4-8) и оборачивать запуск через `asyncio.run(...)`. Это позволит одновременно обрабатывать десятки заметок и сильнее нагружать сервер, контролируя потолок через параметр.

### 3. Разделение I/O и LLM-воркеров
- Чтобы параллельные воркеры не писали в один файл, в `scan_people` завести `asyncio.Queue`.
- LLM-воркеры складывают готовые записи (`note_id`, `PersonNormalizationResponse`) в очередь.
- Отдельный consumer-тайск читает очередь и последовательно пишет `orjson` в `cache/persons_local_normalized.jsonl`, что избавляет от гонок и упрощает логику ретраев.
- При ошибке обработки заметки логировать и продолжать, чтобы очередь не висла (важно при большом числе одновременных задач).

### 4. Асинхронный матчинг кандидатов
- `cluster_people` при построении пар (функции `block_pairs` + `match_candidates`) сейчас вызывает LLM строго последовательно (`src/tools/cluster_people.py:35-48`). После появления `match_candidates_async` можно обрабатывать батч пар параллельно.
- Реализовать очередь пар и пул задач с семафором (например, 10 одновременных запросов). По завершении каждого решения сразу вызывать `uf.union(...)`.
- Это даёт существенный выигрыш, когда пар сотни и тысячи, и позволяет нагружать сервер матчингом так же эффективно, как и сканированием людей.

### 5. Кэширование промежуточных стадий
- Сейчас `normalize_people_in_file` всегда повторно вызывает `extract_note_metadata_from_file` и `extract_people_from_text`. Ввести отдельные JSONL-кэши для каждой стадии (метаданные, raw people, нормализация).
- Структура: `cache/meta/{note_id}.json`, `cache/people_raw/{note_id}.json`, `cache/people_normalized/{note_id}.json`.
- Параллельный пайплайн можно построить как «stage 1 воркеры» (только метаданные), «stage 2» (people extractor), «stage 3» (нормализация). Если какой-то шаг упал, не надо пересчитывать предыдущее — стартуем сразу с недостающей стадии.
- Это также позволяет повторно использовать результаты при кластеризации/линковке без повторных LLM-вызовов и лучше распределять нагрузку (например, в часы простоя пересчитывать только нормализацию).

### 6. Сокращение количества пар для матчингa
- Текущий `block_pairs` выдаёт 1+ млн пар для LLM, что делает `cluster_people` очень долгим (десятки часов). Нужно усилить фильтрацию: учитывать полные `name_parts`, требовать совпадения минимум двух полей, сжимать окно по годам (например, ±20 лет) и игнорировать низкие `confidence`.
- Развить `cheap_decision` — больше детерминированных правил `same/different_person` (совпавшие surface_forms + близкие годы, расхождение имён/отчеств при общей фамилии и т.п.) для снятия нагрузки с LLM.
- Расширить и использовать кэш (`cache/person_match_cache.jsonl`): прогревать его заранее и хранить между прогонами, чтобы новые запуски вызывали модель только для неизвестных пар.
- При необходимости разбить матчинг на батчи или применять более строгий blocking (например, LSH по фамилии+инициалам) — лучше уменьшить количество пар до нескольких десятков тысяч ещё до вызова модели.
